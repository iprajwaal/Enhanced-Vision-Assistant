{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.6.1 (SDL 2.28.4, Python 3.12.4)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import queue\n",
    "import threading\n",
    "import signal\n",
    "import vertexai\n",
    "from google.cloud import vision_v1\n",
    "from google.cloud import texttospeech\n",
    "import cv2\n",
    "import time\n",
    "import numpy as np\n",
    "import pygame\n",
    "from vertexai.preview.generative_models import GenerativeModel\n",
    "from scipy.spatial import distance\n",
    "from typing import List, Dict, Tuple\n",
    "from dataclasses import dataclass\n",
    "from enum import Enum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NavigationPriority(Enum):\n",
    "    CRITICAL = 1    # Immediate collision risk\n",
    "    HIGH = 2        # Hazardous objects or close obstacles\n",
    "    MEDIUM = 3      # Notable obstacles at moderate distance\n",
    "    LOW = 4         # Background information\n",
    "    IGNORE = 5      # Not worth mentioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DetectedObject:\n",
    "    name: str\n",
    "    bbox: List[vision_v1.NormalizedVertex]\n",
    "    confidence: float\n",
    "    depth_estimate: float\n",
    "    priority: NavigationPriority\n",
    "    trajectory: Tuple[float, float] = (0, 0)  # x, y movement\n",
    "    last_seen: float = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MovementTracker:\n",
    "    def __init__(self):\n",
    "        self.movement_history = []\n",
    "        self.last_position = None\n",
    "        \n",
    "    def update_position(self, position):\n",
    "        if self.last_position:\n",
    "            movement = {\n",
    "                'from': self.last_position,\n",
    "                'to': position,\n",
    "                'timestamp': time.time()\n",
    "            }\n",
    "            self.movement_history.append(movement)\n",
    "        self.last_position = position\n",
    "        \n",
    "    def get_recent_movement(self):\n",
    "        # Return last 5 movements\n",
    "        return self.movement_history[-5:] if self.movement_history else []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NavigationContext:\n",
    "    def __init__(self):\n",
    "        self.known_obstacles: Dict[str, DetectedObject] = {}\n",
    "        self.danger_zones: List[Tuple[float, float, float]] = []  # x, y, radius\n",
    "        self.safe_paths: List[Tuple[float, float]] = []\n",
    "        self.user_movement = {\"speed\": 0, \"direction\": 0}\n",
    "        self.last_guidance_time = 0\n",
    "        self.environment_type = \"unknown\"\n",
    "        \n",
    "    def update(self, new_objects: List[DetectedObject]):\n",
    "        current_time = time.time()\n",
    "        # Update known obstacles with new information\n",
    "        for obj in new_objects:\n",
    "            if obj.name in self.known_obstacles:\n",
    "                old_obj = self.known_obstacles[obj.name]\n",
    "                # Calculate object movement\n",
    "                old_center = self.get_object_center(old_obj.bbox)\n",
    "                new_center = self.get_object_center(obj.bbox)\n",
    "                trajectory = (new_center[0] - old_center[0], new_center[1] - old_center[1])\n",
    "                obj.trajectory = trajectory\n",
    "            self.known_obstacles[obj.name] = obj\n",
    "            \n",
    "        # Clean up old obstacles\n",
    "        self.known_obstacles = {k: v for k, v in self.known_obstacles.items() \n",
    "                              if current_time - v.last_seen < 10}  # 10 second timeout\n",
    "        \n",
    "        # Update danger zones based on current obstacles\n",
    "        self.update_danger_zones()\n",
    "        \n",
    "    def get_object_center(self, bbox):\n",
    "        return ((bbox[0].x + bbox[2].x) / 2, (bbox[0].y + bbox[2].y) / 2)\n",
    "        \n",
    "    def update_danger_zones(self):\n",
    "        self.danger_zones = []\n",
    "        for obj in self.known_obstacles.values():\n",
    "            center = self.get_object_center(obj.bbox)\n",
    "            # Calculate danger zone radius based on object size and type\n",
    "            size = (obj.bbox[2].x - obj.bbox[0].x) * (obj.bbox[2].y - obj.bbox[0].y)\n",
    "            radius = size * (1.5 if obj.priority == NavigationPriority.CRITICAL else 1.0)\n",
    "            self.danger_zones.append((center[0], center[1], radius))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentMind:\n",
    "    def __init__(self):\n",
    "        self.context = NavigationContext()\n",
    "        self.hazard_weights = {\n",
    "            'stairs': 1.0, 'hole': 1.0, 'edge': 1.0,\n",
    "            'glass': 0.9, 'knife': 0.9, 'fire': 1.0,\n",
    "            'chair': 0.7, 'table': 0.7, 'person': 0.6,\n",
    "            'wall': 0.8, 'door': 0.6, 'furniture': 0.7\n",
    "        }\n",
    "        \n",
    "    def analyze_scene(self, objects: List[DetectedObject]) -> Dict:\n",
    "        \"\"\"Analyze the scene and make intelligent decisions about navigation\"\"\"\n",
    "        analysis = {\n",
    "            'immediate_threats': [],\n",
    "            'potential_hazards': [],\n",
    "            'safe_paths': [],\n",
    "            'guidance_priority': NavigationPriority.LOW,\n",
    "            'recommended_action': None\n",
    "        }\n",
    "        \n",
    "        # Update navigation context\n",
    "        self.context.update(objects)\n",
    "        \n",
    "        # Identify immediate threats\n",
    "        for obj in objects:\n",
    "            if self.is_immediate_threat(obj):\n",
    "                analysis['immediate_threats'].append(obj)\n",
    "                analysis['guidance_priority'] = NavigationPriority.CRITICAL\n",
    "                \n",
    "        # Find safe paths\n",
    "        safe_paths = self.identify_safe_paths(objects)\n",
    "        if safe_paths:\n",
    "            analysis['safe_paths'] = safe_paths\n",
    "            \n",
    "        # Determine recommended action\n",
    "        analysis['recommended_action'] = self.determine_best_action(analysis)\n",
    "        \n",
    "        return analysis\n",
    "    \n",
    "    def is_immediate_threat(self, obj: DetectedObject) -> bool:\n",
    "        \"\"\"Determine if an object poses an immediate threat\"\"\"\n",
    "        bbox = obj.bbox\n",
    "        height = bbox[2].y - bbox[0].y\n",
    "        width = bbox[2].x - bbox[0].x\n",
    "        center_x = (bbox[0].x + bbox[2].x) / 2\n",
    "        \n",
    "        conditions = [\n",
    "            height > 0.4 and 0.3 < center_x < 0.7,  # Large object directly ahead\n",
    "            obj.name.lower() in self.hazard_weights and self.hazard_weights[obj.name.lower()] > 0.8,\n",
    "            obj.depth_estimate < 1.5 and width > 0.3,  # Very close wide object\n",
    "            any(self.is_moving_towards_user(o) for o in [obj])  # Object moving towards user\n",
    "        ]\n",
    "        \n",
    "        return any(conditions)\n",
    "    \n",
    "    def is_moving_towards_user(self, obj: DetectedObject) -> bool:\n",
    "        \"\"\"Check if object is moving towards the user\"\"\"\n",
    "        if not obj.trajectory:\n",
    "            return False\n",
    "        return obj.trajectory[1] > 0.1  # Threshold for significant movement\n",
    "    \n",
    "    def identify_safe_paths(self, objects: List[DetectedObject]) -> List[str]:\n",
    "        \"\"\"Identify safe navigation paths\"\"\"\n",
    "        safe_paths = []\n",
    "        sectors = {\n",
    "            'left': {'clear': True, 'score': 0},\n",
    "            'center': {'clear': True, 'score': 0},\n",
    "            'right': {'clear': True, 'score': 0}\n",
    "        }\n",
    "        \n",
    "        for obj in objects:\n",
    "            center_x = (obj.bbox[0].x + obj.bbox[2].x) / 2\n",
    "            if center_x < 0.33:\n",
    "                sectors['left']['clear'] = False\n",
    "                sectors['left']['score'] += self.calculate_obstacle_score(obj)\n",
    "            elif center_x < 0.66:\n",
    "                sectors['center']['clear'] = False\n",
    "                sectors['center']['score'] += self.calculate_obstacle_score(obj)\n",
    "            else:\n",
    "                sectors['right']['clear'] = False\n",
    "                sectors['right']['score'] += self.calculate_obstacle_score(obj)\n",
    "        \n",
    "        for sector, data in sectors.items():\n",
    "            if data['clear'] or data['score'] < 0.5:\n",
    "                safe_paths.append(sector)\n",
    "        \n",
    "        return safe_paths\n",
    "    \n",
    "    def calculate_obstacle_score(self, obj: DetectedObject) -> float:\n",
    "        \"\"\"Calculate how much an obstacle blocks a path\"\"\"\n",
    "        size = (obj.bbox[2].y - obj.bbox[0].y) * (obj.bbox[2].x - obj.bbox[0].x)\n",
    "        hazard_weight = self.hazard_weights.get(obj.name.lower(), 0.5)\n",
    "        depth_factor = 1 / max(obj.depth_estimate, 0.1)\n",
    "        return size * hazard_weight * depth_factor\n",
    "    \n",
    "    def determine_best_action(self, analysis: Dict) -> str:\n",
    "        \"\"\"Determine the best action based on scene analysis\"\"\"\n",
    "        if analysis['immediate_threats']:\n",
    "            threat = analysis['immediate_threats'][0]\n",
    "            center_x = (threat.bbox[0].x + threat.bbox[2].x) / 2\n",
    "            return \"Stop and step right\" if center_x < 0.5 else \"Stop and step left\"\n",
    "        \n",
    "        if analysis['safe_paths']:\n",
    "            if 'center' in analysis['safe_paths']:\n",
    "                return \"Proceed straight ahead carefully\"\n",
    "            elif 'left' in analysis['safe_paths']:\n",
    "                return \"Turn slightly left and proceed\"\n",
    "            elif 'right' in analysis['safe_paths']:\n",
    "                return \"Turn slightly right and proceed\"\n",
    "        \n",
    "        return \"Stop and wait for assistance\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnhancedVisionAssistant:\n",
    "    def __init__(self):\n",
    "        # Initialize pygame mixer for audio\n",
    "        pygame.mixer.init(buffer=512)\n",
    "        \n",
    "        # Configuration\n",
    "        self.PROJECT_ID = 'central-rush-447806-r8'\n",
    "        self.REGION = 'us-central1'\n",
    "        self.CREDENTIALS_PATH = '/Users/prajwal/Developer/Enhanced-Vision-Assistant/credentials/key.json'\n",
    "        \n",
    "        # Initialize state variables\n",
    "        self.running = False\n",
    "        self.is_speaking = False\n",
    "        self.cap = None\n",
    "        self.audio_thread = None\n",
    "        self.previous_guidance = None\n",
    "        self.movement_tracker = MovementTracker()\n",
    "        \n",
    "        # Initialize queues and clients\n",
    "        self.audio_queue = queue.PriorityQueue()\n",
    "        os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = self.CREDENTIALS_PATH\n",
    "        vertexai.init(project=self.PROJECT_ID, location=self.REGION)\n",
    "        self.vision_client = vision_v1.ImageAnnotatorClient()\n",
    "        self.speech_client = texttospeech.TextToSpeechClient()\n",
    "        self.model = GenerativeModel(\"gemini-pro\")\n",
    "        \n",
    "        # Initialize the agent mind\n",
    "        self.agent = AgentMind()\n",
    "        \n",
    "        # Configuration parameters\n",
    "        self.DETECTION_INTERVAL = 3  # seconds\n",
    "        self.MIN_OBJECT_SIZE = 0.1\n",
    "\n",
    "    def detect_objects_with_depth(self, frame):\n",
    "        \"\"\"Detect objects and estimate their depth\"\"\"\n",
    "        try:\n",
    "            success, buffer = cv2.imencode('.jpg', frame)\n",
    "            content = buffer.tobytes()\n",
    "            \n",
    "            image = vision_v1.Image(content=content)\n",
    "            features = [\n",
    "                vision_v1.Feature(type=vision_v1.Feature.Type.OBJECT_LOCALIZATION),\n",
    "                vision_v1.Feature(type=vision_v1.Feature.Type.LABEL_DETECTION)\n",
    "            ]\n",
    "            \n",
    "            request = vision_v1.AnnotateImageRequest(image=image, features=features)\n",
    "            response = self.vision_client.annotate_image(request=request)\n",
    "            \n",
    "            detected_objects = []\n",
    "            for obj in response.localized_object_annotations:\n",
    "                bbox = obj.bounding_poly.normalized_vertices\n",
    "                height = bbox[2].y - bbox[0].y\n",
    "                width = bbox[2].x - bbox[0].x\n",
    "                \n",
    "                if height * width > self.MIN_OBJECT_SIZE:\n",
    "                    depth_estimate = 1 / (height * width)\n",
    "                    priority = self.calculate_priority(obj, bbox, depth_estimate)\n",
    "                    \n",
    "                    detected_obj = DetectedObject(\n",
    "                        name=obj.name,\n",
    "                        bbox=bbox,\n",
    "                        confidence=obj.score,\n",
    "                        depth_estimate=depth_estimate,\n",
    "                        priority=priority\n",
    "                    )\n",
    "                    detected_objects.append(detected_obj)\n",
    "            \n",
    "            return detected_objects\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Object detection error: {e}\")\n",
    "            return []\n",
    "\n",
    "    def calculate_priority(self, obj, bbox, depth_estimate):\n",
    "        \"\"\"Calculate priority based on object properties\"\"\"\n",
    "        if self.agent.is_immediate_threat(DetectedObject(\n",
    "            name=obj.name, bbox=bbox, confidence=obj.score,\n",
    "            depth_estimate=depth_estimate, priority=NavigationPriority.LOW\n",
    "        )):\n",
    "            return NavigationPriority.CRITICAL\n",
    "            \n",
    "        height = bbox[2].y - bbox[0].y\n",
    "        center_x = (bbox[0].x + bbox[2].x) / 2\n",
    "        \n",
    "        if height > 0.4 or obj.name.lower() in self.agent.hazard_weights:\n",
    "            return NavigationPriority.HIGH\n",
    "        elif 0.3 < center_x < 0.7 and height > 0.2:\n",
    "            return NavigationPriority.MEDIUM\n",
    "        elif height > 0.1:\n",
    "            return NavigationPriority.LOW\n",
    "        return NavigationPriority.IGNORE\n",
    "\n",
    "    def generate_smart_guidance(self, objects):\n",
    "        \"\"\"Generate intelligent guidance using scene analysis\"\"\"\n",
    "        try:\n",
    "            if not objects:\n",
    "                return None\n",
    "\n",
    "            # Generate scene description\n",
    "            scene_description = []\n",
    "            for obj in objects:\n",
    "                description = self.generate_enhanced_object_description(obj, obj.bbox)\n",
    "                scene_description.append(description)\n",
    "\n",
    "            # Update context\n",
    "            context = {\n",
    "                'previous_guidance': self.previous_guidance,\n",
    "                'movement_history': self.movement_tracker.get_recent_movement(),\n",
    "                'environment_type': 'indoor',  # This could be detected\n",
    "                'current_speed': 'normal',\n",
    "                'recent_obstacles': [obj.name for obj in objects],\n",
    "                'safe_zones': [],\n",
    "                'last_guidance_time': time.time()\n",
    "            }\n",
    "\n",
    "            # Generate and send prompt to AI model\n",
    "            prompt = self.generate_agent_prompt(scene_description, context)\n",
    "            response = self.model.generate_content(prompt)\n",
    "\n",
    "            # Store guidance for future context\n",
    "            self.previous_guidance = response.text\n",
    "            return response.text\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Guidance generation error: {e}\")\n",
    "            return self.generate_fallback_guidance(objects)\n",
    "\n",
    "    def generate_enhanced_object_description(self, obj, bbox):\n",
    "        \"\"\"Generate detailed object description\"\"\"\n",
    "        center_x = (bbox[0].x + bbox[2].x) / 2\n",
    "        position = self.calculate_relative_position(center_x)\n",
    "        distance = self.estimate_distance(bbox[2].y - bbox[0].y, bbox[2].x - bbox[0].x)\n",
    "        \n",
    "        description = f\"{obj.name} {position} at {distance}\"\n",
    "        return description\n",
    "\n",
    "    def calculate_relative_position(self, center_x):\n",
    "        \"\"\"Calculate relative position of object\"\"\"\n",
    "        if center_x < 0.2:\n",
    "            return \"far left\"\n",
    "        elif center_x < 0.4:\n",
    "            return \"to your left\"\n",
    "        elif center_x < 0.6:\n",
    "            return \"directly ahead\"\n",
    "        elif center_x < 0.8:\n",
    "            return \"to your right\"\n",
    "        else:\n",
    "            return \"far right\"\n",
    "\n",
    "    def estimate_distance(self, height, width):\n",
    "        \"\"\"Estimate distance using object dimensions\"\"\"\n",
    "        area = height * width\n",
    "        if area > 0.5:\n",
    "            return \"very close\"\n",
    "        elif area > 0.3:\n",
    "            return \"close\"\n",
    "        elif area > 0.1:\n",
    "            return \"moderate distance\"\n",
    "        else:\n",
    "            return \"far ahead\"\n",
    "\n",
    "    def generate_agent_prompt(self, scene_description, context):\n",
    "        \"\"\"Generate a sophisticated prompt for the AI agent\"\"\"\n",
    "        prompt = f\"\"\"You are an intelligent navigation assistant for a visually impaired person. Your role is to be their eyes and ensure their safety while helping them navigate their environment. Think carefully through each step before providing guidance.\n",
    "\n",
    "            Current Scene Analysis:\n",
    "            1. Detected Objects: {', '.join(scene_description)}\n",
    "            2. Previous Context: {context.get('previous_guidance', 'No previous guidance')}\n",
    "            3. Movement History: {context.get('movement_history', 'Starting navigation')}\n",
    "            4. Environment Type: {context.get('environment_type', 'Unknown')}\n",
    "\n",
    "            Step-by-step Thinking Process:\n",
    "            1. First, analyze immediate safety threats:\n",
    "            - Are there any obstacles in imminent collision path?\n",
    "            - Are there any hazardous objects nearby?\n",
    "            - Is there any moving object approaching?\n",
    "\n",
    "            2. Then, evaluate navigation options:\n",
    "            - Which paths are completely clear?\n",
    "            - What is the safest direction to move?\n",
    "            - Are there any stable objects that could serve as landmarks?\n",
    "\n",
    "            3. Consider environmental context:\n",
    "            - Is this an indoor or outdoor space?\n",
    "            - Are there any recognizable features for orientation?\n",
    "            - Are there any potential changes in elevation (steps, curbs)?\n",
    "\n",
    "            4. Think about user comfort:\n",
    "            - How can they move most confidently?\n",
    "            - What landmarks can help them maintain orientation?\n",
    "            - How urgent is the guidance needed?\n",
    "\n",
    "            Based on this analysis, provide:\n",
    "            1. A primary safety alert if needed (most urgent threats)\n",
    "            2. Clear directional guidance (where to move)\n",
    "            3. Contextual information (what to expect in that direction)\n",
    "\n",
    "            Requirements for your response:\n",
    "            - Use natural, conversational language suitable for text-to-speech\n",
    "            - Be concise but thorough (maximum 3 sentences)\n",
    "            - Start with any urgent warnings\n",
    "            - Use clear spatial references (left, right, ahead, behind)\n",
    "            - Mention distances in practical terms (very close, nearby, ahead)\n",
    "            - If relevant, include time-sensitive information (approaching object, changing conditions)\n",
    "\n",
    "            Additional Context:\n",
    "            - Person's current speed: {context.get('current_speed', 'unknown')}\n",
    "            - Recent obstacles: {context.get('recent_obstacles', [])}\n",
    "            - Known safe zones: {context.get('safe_zones', [])}\n",
    "            - Last guidance timestamp: {context.get('last_guidance_time', 'initial')}\n",
    "\n",
    "            Give your response in a clear, calming voice that prioritizes safety while maintaining user confidence.\n",
    "            \"\"\"\n",
    "        return prompt\n",
    "    \n",
    "    def generate_fallback_guidance(self, objects):\n",
    "        \"\"\"Generate basic guidance when AI model fails\"\"\"\n",
    "        if not objects:\n",
    "            return \"Path appears clear, proceed with caution.\"\n",
    "            \n",
    "        warnings = []\n",
    "        for obj in objects:\n",
    "            description = self.generate_enhanced_object_description(obj, obj.bbox)\n",
    "            warnings.append(description)\n",
    "        \n",
    "        return \". \".join(warnings) + \". Proceed with caution.\"\n",
    "\n",
    "    def process_audio_queue(self):\n",
    "        \"\"\"Process audio queue with priority handling\"\"\"\n",
    "        while self.running:\n",
    "            try:\n",
    "                if not self.is_speaking and not self.audio_queue.empty():\n",
    "                    priority, text = self.audio_queue.get(timeout=1)\n",
    "                    if text:  # Only speak if there's actual guidance\n",
    "                        self.speak(text, priority)\n",
    "                time.sleep(0.1)\n",
    "            except queue.Empty:\n",
    "                continue\n",
    "            except Exception as e:\n",
    "                print(f\"Audio processing error: {e}\")\n",
    "\n",
    "    def speak(self, text, priority=5):\n",
    "        \"\"\"Enhanced text-to-speech with priority handling\"\"\"\n",
    "        try:\n",
    "            self.is_speaking = True\n",
    "            \n",
    "            # Configure voice based on priority\n",
    "            speaking_rate = 1.4 if priority <= 2 else 1.2\n",
    "            \n",
    "            input_text = texttospeech.SynthesisInput(text=text)\n",
    "            voice = texttospeech.VoiceSelectionParams(\n",
    "                language_code=\"en-US\",\n",
    "                name=\"en-US-Standard-F\",\n",
    "                ssml_gender=texttospeech.SsmlVoiceGender.FEMALE\n",
    "            )\n",
    "            audio_config = texttospeech.AudioConfig(\n",
    "                audio_encoding=texttospeech.AudioEncoding.MP3,\n",
    "                speaking_rate=speaking_rate,\n",
    "                pitch=1 if priority > 2 else 1.2\n",
    "            )\n",
    "\n",
    "            response = self.speech_client.synthesize_speech(\n",
    "                input=input_text, voice=voice, audio_config=audio_config\n",
    "            )\n",
    "\n",
    "            # Use a temporary file with priority in name\n",
    "            temp_file = f\"temp_audio_p{priority}_{time.time()}.mp3\"\n",
    "            with open(temp_file, \"wb\") as out:\n",
    "                out.write(response.audio_content)\n",
    "            \n",
    "            pygame.mixer.music.load(temp_file)\n",
    "            pygame.mixer.music.play()\n",
    "            \n",
    "            while pygame.mixer.music.get_busy() and self.running:\n",
    "                time.sleep(0.1)\n",
    "            \n",
    "            if os.path.exists(temp_file):\n",
    "                os.remove(temp_file)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Speech synthesis error: {e}\")\n",
    "        finally:\n",
    "            self.is_speaking = False\n",
    "\n",
    "    def display_debug_frame(self, frame, objects):\n",
    "        \"\"\"Display frame with object annotations for debugging\"\"\"\n",
    "        if objects:\n",
    "            for obj in objects:\n",
    "                bbox = obj.bbox\n",
    "                h, w = frame.shape[:2]\n",
    "                pts = np.array([[int(v.x * w), int(v.y * h)] for v in bbox], np.int32)\n",
    "                \n",
    "                # Color based on priority\n",
    "                color = (0, 255, 0)  # Default color\n",
    "                if obj.priority == NavigationPriority.CRITICAL:\n",
    "                    color = (0, 0, 255)  # Red for critical\n",
    "                elif obj.priority == NavigationPriority.HIGH:\n",
    "                    color = (0, 165, 255)  # Orange for high priority\n",
    "                \n",
    "                cv2.polylines(frame, [pts], True, color, 2)\n",
    "                cv2.putText(frame, f\"{obj.name} (P:{obj.priority.value})\", \n",
    "                          (int(bbox[0].x * w), int(bbox[0].y * h) - 10),\n",
    "                          cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
    "        \n",
    "        cv2.imshow('Enhanced Vision Assistant View', frame)\n",
    "\n",
    "    def start(self):\n",
    "        \"\"\"Start the enhanced vision assistant\"\"\"\n",
    "        try:\n",
    "            print(\"Starting Enhanced Vision Assistant...\")\n",
    "            self.running = True\n",
    "            \n",
    "            # Start audio processing thread\n",
    "            self.audio_thread = threading.Thread(target=self.process_audio_queue)\n",
    "            self.audio_thread.daemon = True\n",
    "            self.audio_thread.start()\n",
    "            \n",
    "            # Initialize camera\n",
    "            self.cap = cv2.VideoCapture(0)\n",
    "            if not self.cap.isOpened():\n",
    "                raise Exception(\"Could not open camera\")\n",
    "            \n",
    "            last_detection_time = 0\n",
    "            \n",
    "            print(\"Press 'q' to quit\")\n",
    "            \n",
    "            while self.running:\n",
    "                ret, frame = self.cap.read()\n",
    "                if not ret:\n",
    "                    break\n",
    "                \n",
    "                current_time = time.time()\n",
    "                \n",
    "                # Perform object detection at intervals\n",
    "                if current_time - last_detection_time >= self.DETECTION_INTERVAL:\n",
    "                    objects = self.detect_objects_with_depth(frame)\n",
    "                    \n",
    "                    if objects:\n",
    "                        guidance = self.generate_smart_guidance(objects)\n",
    "                        if guidance:  # Only queue if there's meaningful guidance\n",
    "                            # Determine priority based on scene analysis\n",
    "                            analysis = self.agent.analyze_scene(objects)\n",
    "                            priority = analysis['guidance_priority'].value\n",
    "                            self.audio_queue.put((priority, guidance))\n",
    "                    \n",
    "                    last_detection_time = current_time\n",
    "                \n",
    "                # Display frame with annotations (debug view)\n",
    "                self.display_debug_frame(frame, objects)\n",
    "                \n",
    "                if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                    break\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"Error in main loop: {e}\")\n",
    "        finally:\n",
    "            self.stop()\n",
    "\n",
    "    def stop(self):\n",
    "        \"\"\"Stop the assistant and cleanup\"\"\"\n",
    "        print(\"Stopping Enhanced Vision Assistant...\")\n",
    "        self.running = False\n",
    "        \n",
    "        # Clear audio queue\n",
    "        while not self.audio_queue.empty():\n",
    "            try:\n",
    "                self.audio_queue.get_nowait()\n",
    "            except queue.Empty:\n",
    "                break\n",
    "        \n",
    "        # Cleanup resources\n",
    "        pygame.mixer.music.stop()\n",
    "        pygame.mixer.quit()\n",
    "        \n",
    "        if self.cap is not None:\n",
    "            self.cap.release()\n",
    "        \n",
    "        cv2.destroyAllWindows()\n",
    "        \n",
    "        if self.audio_thread is not None and self.audio_thread.is_alive():\n",
    "            self.audio_thread.join(timeout=2)\n",
    "        \n",
    "        # Clean up temporary files\n",
    "        for file in os.listdir():\n",
    "            if file.startswith(\"temp_audio_\") and file.endswith(\".mp3\"):\n",
    "                try:\n",
    "                    os.remove(file)\n",
    "                except:\n",
    "                    pass\n",
    "        \n",
    "        print(\"Enhanced Vision Assistant stopped.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    assistant = EnhancedVisionAssistant()\n",
    "    try:\n",
    "        assistant.start()\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nKeyboard interrupt received\")\n",
    "    finally:\n",
    "        assistant.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Enhanced Vision Assistant...\n",
      "Press 'q' to quit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-15 00:03:54.669 Python[82365:4864369] +[IMKClient subclass]: chose IMKClient_Modern\n",
      "2025-02-15 00:03:54.669 Python[82365:4864369] +[IMKInputSession subclass]: chose IMKInputSession_Modern\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Guidance generation error: 429 Quota exceeded for aiplatform.googleapis.com/generate_content_requests_per_minute_per_project_per_base_model with base model: gemini-pro. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/generative-ai/quotas-genai.\n",
      "Guidance generation error: 429 Quota exceeded for aiplatform.googleapis.com/generate_content_requests_per_minute_per_project_per_base_model with base model: gemini-pro. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/generative-ai/quotas-genai.\n",
      "Guidance generation error: 429 Quota exceeded for aiplatform.googleapis.com/generate_content_requests_per_minute_per_project_per_base_model with base model: gemini-pro. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/generative-ai/quotas-genai.\n",
      "Stopping Enhanced Vision Assistant...\n",
      "Enhanced Vision Assistant stopped.\n",
      "\n",
      "Keyboard interrupt received\n",
      "Stopping Enhanced Vision Assistant...\n"
     ]
    },
    {
     "ename": "error",
     "evalue": "mixer not initialized",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m----> 2\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[8], line 8\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mKeyboard interrupt received\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m----> 8\u001b[0m     \u001b[43massistant\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[7], line 372\u001b[0m, in \u001b[0;36mEnhancedVisionAssistant.stop\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    369\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    371\u001b[0m \u001b[38;5;66;03m# Cleanup resources\u001b[39;00m\n\u001b[0;32m--> 372\u001b[0m \u001b[43mpygame\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmixer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmusic\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    373\u001b[0m pygame\u001b[38;5;241m.\u001b[39mmixer\u001b[38;5;241m.\u001b[39mquit()\n\u001b[1;32m    375\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcap \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31merror\u001b[0m: mixer not initialized"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
